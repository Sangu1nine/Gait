import os
import numpy as np
import pickle
from datetime import datetime
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import classification_report, confusion_matrix, f1_score, balanced_accuracy_score
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.model_selection import GroupKFold
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import warnings
import glob
import json
import random
warnings.filterwarnings('ignore')

# TensorFlow 2.10.0 ì„¤ì •
print(f"TensorFlow version: {tf.__version__}")
# TFLite í˜¸í™˜ì„±ì„ ìœ„í•œ ì„¤ì •
tf.random.set_seed(42)
np.random.seed(42)
tf.config.experimental.enable_op_determinism()

# ==== ë‚´ì¥ ì‹œê°í™” ëª¨ë“ˆ ====================================================
import itertools
from sklearn.metrics import (precision_recall_curve, auc)
from sklearn.calibration import calibration_curve
from sklearn.metrics import ConfusionMatrixDisplay

def create_figs_dir():
    os.makedirs("figs", exist_ok=True)

def lr_schedule_plot(lrs, save="figs/lr_schedule.png"):
    plt.figure(figsize=(10, 6))
    plt.plot(lrs)
    plt.title("Learning Rate Schedule")
    plt.xlabel("Training Step")
    plt.ylabel("Learning Rate")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(save, dpi=300)
    plt.close('all')

def pr_curves(y_true, y_prob, class_names, save="figs/pr_curve.png"):
    plt.figure(figsize=(8, 6))
    for cls_idx, cls in enumerate(class_names):
        if len(class_names) == 2:  # Binary classification
            if cls_idx == 1:  # Only plot for positive class
                precision, recall, _ = precision_recall_curve(y_true, y_prob)
                plt.plot(recall, precision, 
                        label=f"{cls} (AUPRC={auc(recall, precision):.3f})")
        else:  # Multi-class
            mask = (y_true == cls_idx)
            precision, recall, _ = precision_recall_curve(mask, y_prob[:, cls_idx])
            plt.plot(recall, precision,
                    label=f"{cls} (AUPRC={auc(recall, precision):.3f})")
    plt.title("Precision-Recall Curves")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(save, dpi=300)
    plt.close('all')

def reliability_diag(y_true, y_prob, n_bins=10, save="figs/calibration_curve.png"):
    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins, strategy="uniform")
    plt.figure(figsize=(8, 6))
    plt.plot(prob_pred, prob_true, "o-", label="Model")
    plt.plot([0, 1], [0, 1], "--", alpha=0.5, label="Perfect Calibration")
    plt.title("Reliability Diagram")
    plt.xlabel("Mean Predicted Probability")
    plt.ylabel("True Fraction of Positives")
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(save, dpi=300)
    plt.close('all')

def normalized_cm(y_true, y_pred, labels, save="figs/confusion_norm.png"):
    cm = confusion_matrix(y_true, y_pred, normalize="true")
    plt.figure(figsize=(8, 6))
    disp = ConfusionMatrixDisplay(cm, display_labels=labels)
    disp.plot(cmap="Blues", values_format=".2f", colorbar=True)
    plt.title("Normalized Confusion Matrix")
    plt.tight_layout()
    plt.savefig(save, dpi=300)
    plt.close('all')

def fold_violin(metrics_dict, metric_name="F1 Score", save="figs/fold_violin.png"):
    data = list(metrics_dict.values())
    plt.figure(figsize=(10, 6))
    sns.violinplot(data=[data])
    plt.xticks(range(len(metrics_dict)), [f"Fold {i+1}" for i in range(len(metrics_dict))])
    plt.ylabel(metric_name)
    plt.title(f"{metric_name} Distribution Across Folds")
    plt.tight_layout()
    plt.savefig(save, dpi=300)
    plt.close('all')

def subject_heatmap(recall_matrix, subj_ids, class_names, save="figs/subject_heatmap.png"):
    plt.figure(figsize=(len(class_names)*1.5, len(subj_ids)*0.4+2))
    sns.heatmap(recall_matrix, annot=True, fmt=".2f",
                yticklabels=subj_ids, xticklabels=class_names,
                cmap="YlGnBu", cbar_kws={"label": "Recall"})
    plt.xlabel("Class")
    plt.ylabel("Subject")
    plt.title("Per-Subject Recall Performance")
    plt.tight_layout()
    plt.savefig(save, dpi=300)
    plt.close('all')

def scaler_comparison_plot(standard_results, minmax_results, save="figs/scaler_comparison.png"):
    """ë‘ ìŠ¤ì¼€ì¼ëŸ¬ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. F1 Score ë¹„êµ
    ax = axes[0, 0]
    standard_f1 = [r['f1_score'] for r in standard_results]
    minmax_f1 = [r['f1_score'] for r in minmax_results]
    
    x = np.arange(len(standard_f1))
    width = 0.35
    ax.bar(x - width/2, standard_f1, width, label='StandardScaler', alpha=0.8)
    ax.bar(x + width/2, minmax_f1, width, label='MinMaxScaler', alpha=0.8)
    ax.set_xlabel('Fold')
    ax.set_ylabel('F1 Score')
    ax.set_title('F1 Score Comparison by Fold')
    ax.set_xticks(x)
    ax.set_xticklabels([f'Fold {i+1}' for i in range(len(standard_f1))])
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 2. Balanced Accuracy ë¹„êµ
    ax = axes[0, 1]
    standard_ba = [r['balanced_accuracy'] for r in standard_results]
    minmax_ba = [r['balanced_accuracy'] for r in minmax_results]
    
    ax.bar(x - width/2, standard_ba, width, label='StandardScaler', alpha=0.8)
    ax.bar(x + width/2, minmax_ba, width, label='MinMaxScaler', alpha=0.8)
    ax.set_xlabel('Fold')
    ax.set_ylabel('Balanced Accuracy')
    ax.set_title('Balanced Accuracy Comparison by Fold')
    ax.set_xticks(x)
    ax.set_xticklabels([f'Fold {i+1}' for i in range(len(standard_ba))])
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 3. í‰ê·  ì„±ëŠ¥ ë¹„êµ
    ax = axes[1, 0]
    metrics = ['F1 Score', 'Balanced Accuracy']
    standard_means = [np.mean(standard_f1), np.mean(standard_ba)]
    minmax_means = [np.mean(minmax_f1), np.mean(minmax_ba)]
    standard_stds = [np.std(standard_f1), np.std(standard_ba)]
    minmax_stds = [np.std(minmax_f1), np.std(minmax_ba)]
    
    x = np.arange(len(metrics))
    ax.bar(x - width/2, standard_means, width, yerr=standard_stds, 
           label='StandardScaler', alpha=0.8, capsize=5)
    ax.bar(x + width/2, minmax_means, width, yerr=minmax_stds,
           label='MinMaxScaler', alpha=0.8, capsize=5)
    ax.set_ylabel('Score')
    ax.set_title('Average Performance Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels(metrics)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 4. ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸”
    ax = axes[1, 1]
    ax.axis('off')
    
    # ì„±ëŠ¥ í†µê³„ ê³„ì‚°
    standard_f1_mean, standard_f1_std = np.mean(standard_f1), np.std(standard_f1)
    minmax_f1_mean, minmax_f1_std = np.mean(minmax_f1), np.std(minmax_f1)
    standard_ba_mean, standard_ba_std = np.mean(standard_ba), np.std(standard_ba)
    minmax_ba_mean, minmax_ba_std = np.mean(minmax_ba), np.std(minmax_ba)
    
    # ìŠ¹ì ê²°ì •
    f1_winner = "StandardScaler" if standard_f1_mean > minmax_f1_mean else "MinMaxScaler"
    ba_winner = "StandardScaler" if standard_ba_mean > minmax_ba_mean else "MinMaxScaler"
    
    table_text = f"""
Performance Summary

F1 Score:
  StandardScaler: {standard_f1_mean:.4f} Â± {standard_f1_std:.4f}
  MinMaxScaler:   {minmax_f1_mean:.4f} Â± {minmax_f1_std:.4f}
  Winner: {f1_winner}

Balanced Accuracy:
  StandardScaler: {standard_ba_mean:.4f} Â± {standard_ba_std:.4f}
  MinMaxScaler:   {minmax_ba_mean:.4f} Â± {minmax_ba_std:.4f}
  Winner: {ba_winner}

Recommendation:
  {"StandardScaler performs better overall" if standard_f1_mean + standard_ba_mean > minmax_f1_mean + minmax_ba_mean else "MinMaxScaler performs better overall"}
    """
    
    ax.text(0.1, 0.9, table_text, transform=ax.transAxes, fontsize=10,
            verticalalignment='top', fontfamily='monospace',
            bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(save, dpi=300, bbox_inches='tight')
    plt.close('all')

# ==== TCN ëª¨ë¸ ì •ì˜ (ê³¼ì í•© í•´ê²° ë²„ì „) ======================================

class TCNBlock(layers.Layer):
    """Temporal Convolutional Network Block (ê³¼ì í•© ë°©ì§€ ê°•í™”)"""
    def __init__(self, filters, kernel_size, dilation_rate, dropout_rate=0.5, **kwargs):
        super(TCNBlock, self).__init__(**kwargs)
        self.filters = filters
        self.kernel_size = kernel_size
        self.dilation_rate = dilation_rate
        self.dropout_rate = dropout_rate

        # main path - ê°•í™”ëœ ì •ê·œí™”
        self.conv1 = layers.Conv1D(
            filters=filters, 
            kernel_size=kernel_size,
            dilation_rate=dilation_rate,
            padding="causal",
            use_bias=True,
            activation=None,
            kernel_regularizer=keras.regularizers.l2(1e-3)  # ê°•í™”ëœ L2 ì •ê·œí™”
        )
        self.conv2 = layers.Conv1D(
            filters=filters, 
            kernel_size=kernel_size,
            dilation_rate=dilation_rate,
            padding="causal",
            use_bias=True,
            activation=None,
            kernel_regularizer=keras.regularizers.l2(1e-3)  # ê°•í™”ëœ L2 ì •ê·œí™”
        )
        # LayerNormalization ëŒ€ì‹  BatchNormalization ì‚¬ìš© (TFLite ìµœì í™”)
        self.batchnorm1 = layers.BatchNormalization()
        self.batchnorm2 = layers.BatchNormalization()
        self.dropout1 = layers.Dropout(dropout_rate)
        self.dropout2 = layers.Dropout(dropout_rate)
        self.activation = layers.ReLU()

        # residual path
        self.residual_conv = None

    def build(self, input_shape):
        super(TCNBlock, self).build(input_shape)
        if input_shape[-1] != self.filters:
            self.residual_conv = layers.Conv1D(
                filters=self.filters, 
                kernel_size=1, 
                padding="same",
                use_bias=True,
                activation=None,
                kernel_regularizer=keras.regularizers.l2(1e-3)  # ê°•í™”ëœ L2 ì •ê·œí™”
            )

    def call(self, inputs, training=False):
        x = self.conv1(inputs)
        x = self.batchnorm1(x, training=training)
        x = self.activation(x)
        x = self.dropout1(x, training=training)

        x = self.conv2(x)
        x = self.batchnorm2(x, training=training)
        x = self.activation(x)
        x = self.dropout2(x, training=training)

        # residual connection
        residual = self.residual_conv(inputs) if self.residual_conv else inputs
        return layers.Add()([residual, x])

    def get_config(self):
        config = super(TCNBlock, self).get_config()
        config.update({
            'filters': self.filters,
            'kernel_size': self.kernel_size,
            'dilation_rate': self.dilation_rate,
            'dropout_rate': self.dropout_rate
        })
        return config

# ==== Stage1 ëª¨ë¸ í´ë˜ìŠ¤ (ê³¼ì í•© í•´ê²° ë²„ì „) =================================

class Stage1Model:
    """Stage 1: Gait/Non-gait Binary Classification (ê³¼ì í•© í•´ê²° ë²„ì „)"""
    def __init__(self, input_shape=(60, 6)):
        self.input_shape = input_shape
        self.model = None
        self.label_encoder = LabelEncoder()
        self.training_history = []
        
    def build_model(self):
        """TCN ëª¨ë¸ êµ¬ì¶• (ê³¼ì í•© ë°©ì§€ ê°•í™”)"""
        inputs = Input(shape=self.input_shape, name='input_layer')
        
        # ğŸ”§ ìˆ˜ì •: TCN layers ìˆ˜ ì¤„ì´ê¸° + í•„í„° ìˆ˜ ê°ì†Œ
        x = TCNBlock(16, kernel_size=3, dilation_rate=1, dropout_rate=0.5, name='tcn_block_1')(inputs)  # 32â†’16
        x = TCNBlock(32, kernel_size=3, dilation_rate=2, dropout_rate=0.5, name='tcn_block_2')(x)      # 64â†’32
        # ğŸ”§ ìˆ˜ì •: ë¸”ë¡ 4ê°œ â†’ 2ê°œë¡œ ì¤„ì„
        
        # Global pooling
        x = layers.GlobalAveragePooling1D(name='global_avg_pool')(x)
        
        # Dense layers - ë” ë‹¨ìˆœí™”
        x = layers.Dense(16, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-3), name='dense_1')(x)  # 32â†’16
        x = layers.Dropout(0.5, name='dropout_final')(x)  # Dropout ìœ ì§€
        outputs = layers.Dense(1, activation='sigmoid', name='output_layer')(x)
        
        self.model = Model(inputs=inputs, outputs=outputs, name='stage1_gait_classifier')
        
        # ğŸ”§ ìˆ˜ì •: í•™ìŠµë¥  ê°ì†Œ
        self.model.compile(
            optimizer=Adam(learning_rate=0.0005),  # 0.001 â†’ 0.0005
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        return self.model
    
    def get_groupkfold_splits(self, X, y, subjects, n_splits=5):
        """GroupKFoldë¥¼ ì‚¬ìš©í•œ Subject-level validation"""
        gkf = GroupKFold(n_splits=n_splits)
        folds = []
        
        for fold_idx, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=subjects)):
            test_subjects = np.unique(subjects[val_idx])
            train_subjects = np.unique(subjects[train_idx])
            
            folds.append({
                'fold': fold_idx,
                'train_idx': train_idx,
                'val_idx': val_idx,
                'test_subjects': test_subjects,
                'train_subjects': train_subjects
            })
            
        return folds
    
    def apply_scaling(self, X_train, X_test, scaler_type='standard'):
        """ìŠ¤ì¼€ì¼ë§ ì ìš© (Data Leakage ë°©ì§€)"""
        # 3D ë°ì´í„°ë¥¼ 2Dë¡œ ë³€í™˜
        n_train_samples, n_timesteps, n_features = X_train.shape
        n_test_samples = X_test.shape[0]
        
        X_train_2d = X_train.reshape(-1, n_features)
        X_test_2d = X_test.reshape(-1, n_features)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì„ íƒ
        if scaler_type == 'standard':
            scaler = StandardScaler()
        elif scaler_type == 'minmax':
            scaler = MinMaxScaler()
        else:
            raise ValueError(f"Unknown scaler_type: {scaler_type}")
        
        # í›ˆë ¨ ë°ì´í„°ë¡œë§Œ fit, í›ˆë ¨+í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— transform
        X_train_scaled = scaler.fit_transform(X_train_2d)
        X_test_scaled = scaler.transform(X_test_2d)  # fit ì—†ì´ transformë§Œ!
        
        # ë‹¤ì‹œ 3Dë¡œ ë³€í™˜
        X_train_scaled = X_train_scaled.reshape(X_train.shape).astype(np.float32)
        X_test_scaled = X_test_scaled.reshape(X_test.shape).astype(np.float32)
        
        return X_train_scaled, X_test_scaled, scaler
    
    def tune_threshold(self, y_true, y_prob):
        """ì„ê³„ê°’ íŠœë‹ìœ¼ë¡œ F1 ìŠ¤ì½”ì–´ ìµœì í™”"""
        best_threshold = 0.5
        best_f1 = 0.0
        
        for threshold in np.linspace(0.20, 0.60, 41):
            y_pred = (y_prob > threshold).astype(int)
            f1 = f1_score(y_true, y_pred)
            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold
                
        return best_threshold, best_f1
    
    def train_with_scaler(self, X, y, subjects, scaler_type='standard'):
        """íŠ¹ì • ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ëª¨ë¸ í•™ìŠµ"""
        print(f"\n=== {scaler_type.upper()}SCALER í•™ìŠµ ì‹œì‘ ===")
        
        # Label encoding
        y_encoded = self.label_encoder.fit_transform(y)
        
        # Get GroupKFold splits
        folds = self.get_groupkfold_splits(X, y_encoded, subjects, n_splits=5)
        
        # Training results
        fold_results = []
        all_y_true = []
        all_y_prob = []
        all_y_pred = []
        fold_metrics = {}
        subject_recalls = []
        fold_thresholds = []
        all_scalers = []  # ê° foldì˜ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        
        for fold_data in folds:
            fold_idx = fold_data['fold']
            train_idx = fold_data['train_idx']
            val_idx = fold_data['val_idx']
            
            print(f"\n--- Fold {fold_idx + 1}/{len(folds)} ({scaler_type.upper()}) ---")
            print(f"Train subjects: {fold_data['train_subjects']}")
            print(f"Test subjects: {fold_data['test_subjects']}")
            
            # Diversify random seeds per fold
            seed = 42 + fold_idx
            os.environ["PYTHONHASHSEED"] = str(seed)
            random.seed(seed)
            np.random.seed(seed)
            tf.random.set_seed(seed)
            
            # Build new model for each fold
            self.build_model()
            
            # Split data
            X_train, y_train = X[train_idx], y_encoded[train_idx]
            X_val, y_val = X[val_idx], y_encoded[val_idx]
            
            # Apply scaling (Data Leakage ë°©ì§€)
            X_train_scaled, X_val_scaled, scaler = self.apply_scaling(
                X_train, X_val, scaler_type=scaler_type)
            all_scalers.append(scaler)
            
            print(f"  Data shape after scaling: Train {X_train_scaled.shape}, Val {X_val_scaled.shape}")
            print(f"  Train data range: [{X_train_scaled.min():.3f}, {X_train_scaled.max():.3f}]")
            print(f"  Val data range: [{X_val_scaled.min():.3f}, {X_val_scaled.max():.3f}]")
            
            # Class weights for imbalanced data
            class_weight = {0: 1.0, 1: len(y_train[y_train==0]) / len(y_train[y_train==1])}
            
            # Callbacks - Early Stopping patience ìœ ì§€
            early_stopping = EarlyStopping(
                monitor='val_loss',
                mode='min',
                patience=15,  # ìœ ì§€
                min_delta=1e-4,
                restore_best_weights=True,
                verbose=1
            )
            
            reduce_lr = ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-6,
                verbose=1
            )
            
            # ğŸ”§ ìˆ˜ì •: ë°°ì¹˜ í¬ê¸° ê°ì†Œ
            history = self.model.fit(
                X_train_scaled, y_train,
                validation_data=(X_val_scaled, y_val),
                epochs=50,
                batch_size=64,  # 256 â†’ 64
                class_weight=class_weight,
                callbacks=[early_stopping, reduce_lr],
                verbose=1
            )
            
            # Evaluate with threshold tuning
            y_pred_prob = self.model.predict(X_val_scaled, batch_size=128, verbose=0)  # ë°°ì¹˜ í¬ê¸° ì¡°ì •
            y_pred_prob_flat = y_pred_prob.flatten()
            
            # Tune threshold
            best_threshold, best_f1_tuned = self.tune_threshold(y_val, y_pred_prob_flat)
            fold_thresholds.append(best_threshold)
            
            # Use tuned threshold for predictions
            y_pred = (y_pred_prob_flat > best_threshold).astype(int)
            
            print(f"  Chosen threshold = {best_threshold:.3f} (F1 = {best_f1_tuned:.3f})")
            
            # Store results for visualization
            all_y_true.extend(y_val)
            all_y_prob.extend(y_pred_prob_flat)
            all_y_pred.extend(y_pred)
            
            # Metrics
            f1 = f1_score(y_val, y_pred, average='macro')
            ba = balanced_accuracy_score(y_val, y_pred)
            
            fold_results.append({
                'fold': fold_idx,
                'f1_score': f1,
                'balanced_accuracy': ba,
                'confusion_matrix': confusion_matrix(y_val, y_pred),
                'history': history,
                'best_threshold': best_threshold,
                'tuned_f1': best_f1_tuned,
                'scaler': scaler,
                'scaler_type': scaler_type
            })
            
            fold_metrics[f'Fold_{fold_idx+1}'] = f1
            
            # Subject-level recall calculation
            test_subjects_fold = subjects[val_idx]
            for subj in np.unique(test_subjects_fold):
                subj_mask = test_subjects_fold == subj
                subj_true = y_val[subj_mask]
                subj_pred = y_pred[subj_mask]
                
                if len(np.unique(subj_true)) > 1:  # Both classes present
                    subj_recall_0 = np.sum((subj_true == 0) & (subj_pred == 0)) / np.sum(subj_true == 0)
                    subj_recall_1 = np.sum((subj_true == 1) & (subj_pred == 1)) / np.sum(subj_true == 1)
                else:  # Only one class
                    if subj_true[0] == 0:
                        subj_recall_0 = np.sum(subj_pred == 0) / len(subj_pred)
                        subj_recall_1 = 0.0
                    else:
                        subj_recall_0 = 0.0
                        subj_recall_1 = np.sum(subj_pred == 1) / len(subj_pred)
                
                subject_recalls.append([subj, subj_recall_0, subj_recall_1])
            
            print(f"  Fold {fold_idx + 1} - F1: {f1:.4f}, Balanced Acc: {ba:.4f}")
        
        # ì„±ëŠ¥ ìš”ì•½
        avg_f1 = np.mean([r['f1_score'] for r in fold_results])
        avg_ba = np.mean([r['balanced_accuracy'] for r in fold_results])
        avg_threshold = np.mean(fold_thresholds)
        
        print(f"\n=== {scaler_type.upper()}SCALER í•™ìŠµ ì™„ë£Œ ===")
        print(f"  í‰ê·  F1 Score: {avg_f1:.4f} Â± {np.std([r['f1_score'] for r in fold_results]):.4f}")
        print(f"  í‰ê·  Balanced Accuracy: {avg_ba:.4f} Â± {np.std([r['balanced_accuracy'] for r in fold_results]):.4f}")
        print(f"  í‰ê·  Threshold: {avg_threshold:.3f}")
        
        return fold_results, all_y_true, all_y_prob, all_y_pred, fold_metrics, subject_recalls, fold_thresholds
    
    def train(self, X, y, subjects, save_path):
        """ë‘ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ëª¨ë¸ í•™ìŠµ ë° ë¹„êµ"""
        print("="*60)
        print("ğŸš€ Stage1 ì´ì¤‘ ìŠ¤ì¼€ì¼ë§ í•™ìŠµ ì‹œì‘ (ê³¼ì í•© í•´ê²° ë²„ì „)")
        print("ğŸ”§ ìˆ˜ì •ì‚¬í•­: TCN ë¸”ë¡ ê°ì†Œ, í•„í„° ìˆ˜ ê°ì†Œ, í•™ìŠµë¥  ê°ì†Œ, ë°°ì¹˜ í¬ê¸° ê°ì†Œ")
        print("="*60)
        
        # figs ë””ë ‰í† ë¦¬ ìƒì„±
        create_figs_dir()
        
        # 1. StandardScalerë¡œ í•™ìŠµ
        standard_results, standard_y_true, standard_y_prob, standard_y_pred, \
        standard_fold_metrics, standard_subject_recalls, standard_thresholds = \
            self.train_with_scaler(X, y, subjects, scaler_type='standard')
        
        # 2. MinMaxScalerë¡œ í•™ìŠµ
        minmax_results, minmax_y_true, minmax_y_prob, minmax_y_pred, \
        minmax_fold_metrics, minmax_subject_recalls, minmax_thresholds = \
            self.train_with_scaler(X, y, subjects, scaler_type='minmax')
        
        # 3. ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”
        print("\n=== ìŠ¤ì¼€ì¼ëŸ¬ ì„±ëŠ¥ ë¹„êµ ===")
        self.compare_scalers(standard_results, minmax_results, save_path)
        
        # 4. ê° ìŠ¤ì¼€ì¼ëŸ¬ë³„ ì‹œê°í™” ìƒì„± ë° ëª¨ë¸ ì €ì¥
        self.save_scaler_results('standard', standard_results, standard_y_true, 
                               standard_y_prob, standard_y_pred, standard_fold_metrics,
                               standard_subject_recalls, standard_thresholds, save_path)
        
        self.save_scaler_results('minmax', minmax_results, minmax_y_true,
                               minmax_y_prob, minmax_y_pred, minmax_fold_metrics,
                               minmax_subject_recalls, minmax_thresholds, save_path)
        
        # 5. ìµœì¢… ì¶”ì²œ
        standard_avg_f1 = np.mean([r['f1_score'] for r in standard_results])
        minmax_avg_f1 = np.mean([r['f1_score'] for r in minmax_results])
        standard_avg_ba = np.mean([r['balanced_accuracy'] for r in standard_results])
        minmax_avg_ba = np.mean([r['balanced_accuracy'] for r in minmax_results])
        
        standard_total = standard_avg_f1 + standard_avg_ba
        minmax_total = minmax_avg_f1 + minmax_avg_ba
        
        recommended_scaler = 'StandardScaler' if standard_total > minmax_total else 'MinMaxScaler'
        
        print("\n" + "="*60)
        print("âœ… ì´ì¤‘ ìŠ¤ì¼€ì¼ë§ í•™ìŠµ ì™„ë£Œ! (ê³¼ì í•© í•´ê²°)")
        print("="*60)
        print(f"ğŸ“Š ìµœì¢… ì„±ëŠ¥ ë¹„êµ:")
        print(f"  StandardScaler: F1={standard_avg_f1:.4f}, BA={standard_avg_ba:.4f}")
        print(f"  MinMaxScaler:   F1={minmax_avg_f1:.4f}, BA={minmax_avg_ba:.4f}")
        print(f"ğŸ† ì¶”ì²œ ìŠ¤ì¼€ì¼ëŸ¬: {recommended_scaler}")
        print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜:")
        print(f"  - model_standard_scaler/")
        print(f"  - model_minmax_scaler/")
        print(f"  - ìŠ¤ì¼€ì¼ëŸ¬ ë¹„êµ ì‹œê°í™”")
        print(f"ğŸ”§ ì ìš©ëœ ê³¼ì í•© í•´ê²°ì±…:")
        print(f"  - TCN ë¸”ë¡: 4ê°œ â†’ 2ê°œ")
        print(f"  - í•„í„° ìˆ˜: [32,32,64,64] â†’ [16,32]")
        print(f"  - Dense í¬ê¸°: 32 â†’ 16")
        print(f"  - í•™ìŠµë¥ : 0.001 â†’ 0.0005")
        print(f"  - ë°°ì¹˜ í¬ê¸°: 256 â†’ 64")
        print(f"  - L2 ì •ê·œí™”: 1e-4 â†’ 1e-3")
        
        return {
            'standard_results': standard_results,
            'minmax_results': minmax_results,
            'recommended_scaler': recommended_scaler,
            'comparison_metrics': {
                'standard_f1': standard_avg_f1,
                'minmax_f1': minmax_avg_f1,
                'standard_ba': standard_avg_ba,
                'minmax_ba': minmax_avg_ba
            }
        }
    
    def compare_scalers(self, standard_results, minmax_results, save_path):
        """ë‘ ìŠ¤ì¼€ì¼ëŸ¬ ì„±ëŠ¥ ë¹„êµ"""
        # ë¹„êµ ì‹œê°í™” ìƒì„±
        scaler_comparison_plot(standard_results, minmax_results, 
                             save=os.path.join(save_path, "figs/scaler_comparison.png"))
        
        # í†µê³„ì  ìœ ì˜ì„± ê²€ì • (ì„ íƒì‚¬í•­)
        from scipy import stats
        
        standard_f1 = [r['f1_score'] for r in standard_results]
        minmax_f1 = [r['f1_score'] for r in minmax_results]
        
        # Paired t-test (ê°™ì€ foldì—ì„œ ë¹„êµ)
        t_stat, p_value = stats.ttest_rel(standard_f1, minmax_f1)
        
        print(f"ğŸ“ˆ í†µê³„ì  ë¹„êµ:")
        print(f"  Paired t-test: t={t_stat:.4f}, p={p_value:.4f}")
        if p_value < 0.05:
            winner = "StandardScaler" if np.mean(standard_f1) > np.mean(minmax_f1) else "MinMaxScaler"
            print(f"  ê²°ê³¼: {winner}ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ê²Œ ìš°ìˆ˜í•¨ (p<0.05)")
        else:
            print(f"  ê²°ê³¼: ë‘ ìŠ¤ì¼€ì¼ëŸ¬ ê°„ ìœ ì˜í•œ ì°¨ì´ ì—†ìŒ (pâ‰¥0.05)")
    
    def save_scaler_results(self, scaler_name, fold_results, y_true, y_prob, y_pred, 
                          fold_metrics, subject_recalls, fold_thresholds, save_path):
        """íŠ¹ì • ìŠ¤ì¼€ì¼ëŸ¬ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_dir = os.path.join(save_path, f"model_{scaler_name}_scaler_{timestamp}")
        os.makedirs(model_dir, exist_ok=True)
        
        # figs ë””ë ‰í† ë¦¬ ìƒì„±
        figs_dir = os.path.join(model_dir, "figs")
        os.makedirs(figs_dir, exist_ok=True)
        
        print(f"\nğŸ’¾ {scaler_name.upper()}SCALER ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # ì‹œê°í™” ìƒì„±
        self.generate_visualizations(y_true, y_prob, y_pred, fold_metrics, 
                                   subject_recalls, figs_dir, scaler_name)
        
        # ë§ˆì§€ë§‰ foldì˜ ëª¨ë¸ ì €ì¥ (ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ foldë¡œ ì„ íƒ ê°€ëŠ¥)
        best_fold_idx = np.argmax([r['f1_score'] for r in fold_results])
        best_fold_result = fold_results[best_fold_idx]
        
        print(f"  ìµœê³  ì„±ëŠ¥ fold: {best_fold_idx + 1} (F1: {best_fold_result['f1_score']:.4f})")
        
        # ìµœê³  ì„±ëŠ¥ foldë¡œ ëª¨ë¸ ì¬í•™ìŠµ (ì „ì²´ ë°ì´í„° ì‚¬ìš©)
        self.save_best_model(best_fold_result, model_dir, scaler_name, fold_thresholds)
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        avg_f1 = np.mean([r['f1_score'] for r in fold_results])
        avg_ba = np.mean([r['balanced_accuracy'] for r in fold_results])
        avg_threshold = np.mean(fold_thresholds)
        
        metrics = {
            'scaler_type': scaler_name,
            'avg_f1_score': avg_f1,
            'avg_balanced_accuracy': avg_ba,
            'avg_threshold': avg_threshold,
            'fold_results': fold_results,
            'fold_thresholds': fold_thresholds,
            'input_shape': self.input_shape,
            'best_fold': best_fold_idx + 1,
            'model_summary': self.model.to_json() if self.model else None,
            'overfitting_fixes': {
                'tcn_blocks_reduced': '4 â†’ 2',
                'filters_reduced': '[32,32,64,64] â†’ [16,32]',
                'dense_size_reduced': '32 â†’ 16',
                'learning_rate_reduced': '0.001 â†’ 0.0005',
                'batch_size_reduced': '256 â†’ 64',
                'l2_regularization_increased': '1e-4 â†’ 1e-3'
            }
        }
        
        with open(os.path.join(model_dir, "metrics.pkl"), 'wb') as f:
            pickle.dump(metrics, f)
        
        # JSONìœ¼ë¡œë„ ì €ì¥ (ì‚¬ëŒì´ ì½ê¸° ì‰½ê²Œ)
        json_metrics = {
            'scaler_type': scaler_name,
            'avg_f1_score': float(avg_f1),
            'avg_balanced_accuracy': float(avg_ba),
            'avg_threshold': float(avg_threshold),
            'best_fold': int(best_fold_idx + 1),
            'overfitting_fixes_applied': True,
            'model_improvements': {
                'reduced_complexity': 'TCN blocks: 4â†’2, Filters: [32,64]â†’[16,32]',
                'regularization': 'L2: 1e-4â†’1e-3, Dropout: 0.5 (maintained)',
                'training': 'LR: 0.001â†’0.0005, Batch: 256â†’64'
            },
            'fold_performance': [
                {
                    'fold': int(r['fold'] + 1),
                    'f1_score': float(r['f1_score']),
                    'balanced_accuracy': float(r['balanced_accuracy']),
                    'threshold': float(r['best_threshold'])
                }
                for r in fold_results
            ]
        }
        
        with open(os.path.join(model_dir, "metrics.json"), 'w', encoding='utf-8') as f:
            json.dump(json_metrics, f, indent=2, ensure_ascii=False)
        
        print(f"  âœ… {scaler_name.upper()}SCALER ì €ì¥ ì™„ë£Œ: {model_dir}")
        print(f"     í‰ê·  F1: {avg_f1:.4f}, í‰ê·  BA: {avg_ba:.4f}")
    
    def save_best_model(self, best_fold_result, model_dir, scaler_name, fold_thresholds):
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥"""
        # ìµœê³  ì„±ëŠ¥ foldì˜ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        scaler = best_fold_result['scaler']
        with open(os.path.join(model_dir, f"{scaler_name}_scaler.pkl"), 'wb') as f:
            pickle.dump(scaler, f)
        
        # ë¼ë²¨ ì¸ì½”ë” ì €ì¥
        with open(os.path.join(model_dir, "label_encoder.pkl"), 'wb') as f:
            pickle.dump(self.label_encoder, f)
        
        # ì„ê³„ê°’ë“¤ ì €ì¥
        thresholds_dict = {
            f'fold_{i+1}': float(threshold) 
            for i, threshold in enumerate(fold_thresholds)
        }
        with open(os.path.join(model_dir, "thresholds.json"), 'w') as f:
            json.dump(thresholds_dict, f, indent=2)
        
        # Keras ëª¨ë¸ ì €ì¥
        if self.model:
            keras_model_path = os.path.join(model_dir, "model.keras")
            self.model.save(keras_model_path)
            
            # SavedModel í˜•ì‹ ì €ì¥
            savedmodel_path = os.path.join(model_dir, "saved_model")
            self.model.save(savedmodel_path, save_format='tf')
            
            # TFLite ë³€í™˜
            self.convert_to_tflite(savedmodel_path, model_dir)
        
        # ì‚¬ìš©ë²• ê°€ì´ë“œ ìƒì„±
        self.create_usage_guide(model_dir, scaler_name)
    
    def create_usage_guide(self, model_dir, scaler_name):
        """ì‚¬ìš©ë²• ê°€ì´ë“œ ìƒì„± (ê³¼ì í•© í•´ê²° ì •ë³´ í¬í•¨)"""
        usage_guide = f'''# {scaler_name.upper()}SCALER ëª¨ë¸ ì‚¬ìš© ê°€ì´ë“œ (ê³¼ì í•© í•´ê²° ë²„ì „)

## ğŸ”§ ì ìš©ëœ ê³¼ì í•© í•´ê²°ì±…
- **TCN ë¸”ë¡ ìˆ˜**: 4ê°œ â†’ 2ê°œ (ë³µì¡ë„ ê°ì†Œ)
- **í•„í„° ìˆ˜**: [32,32,64,64] â†’ [16,32] (íŒŒë¼ë¯¸í„° ê°ì†Œ)
- **Dense ë ˆì´ì–´**: 32 â†’ 16 (ìš©ëŸ‰ ê°ì†Œ)
- **í•™ìŠµë¥ **: 0.001 â†’ 0.0005 (ì•ˆì •ì  í•™ìŠµ)
- **ë°°ì¹˜ í¬ê¸°**: 256 â†’ 64 (ì¼ë°˜í™” í–¥ìƒ)
- **L2 ì •ê·œí™”**: 1e-4 â†’ 1e-3 (ê³¼ì í•© ì–µì œ ê°•í™”)
- **Dropout**: 0.5 ìœ ì§€ (ê¸°ì¡´ ìš”ì²­ì— ë”°ë¼)
- **Early Stopping**: patience=15 ìœ ì§€ (ê¸°ì¡´ ìš”ì²­ì— ë”°ë¼)

## íŒŒì¼ êµ¬ì¡°
- model.keras: Keras ëª¨ë¸ (ê³¼ì í•© í•´ê²°)
- saved_model/: TensorFlow SavedModel
- model.tflite: TensorFlow Lite ëª¨ë¸
- {scaler_name}_scaler.pkl: {scaler_name.upper()}Scaler ê°ì²´
- label_encoder.pkl: ë¼ë²¨ ì¸ì½”ë”
- thresholds.json: ìµœì  ì„ê³„ê°’ë“¤
- metrics.json: ì„±ëŠ¥ ë©”íŠ¸ë¦­ (ê°œì„ ì‚¬í•­ í¬í•¨)

## ì‚¬ìš© ì˜ˆì‹œ

```python
import numpy as np
import pickle
import tensorflow as tf
from sklearn.preprocessing import {"StandardScaler" if scaler_name == "standard" else "MinMaxScaler"}

# 1. ëª¨ë¸ ë° ì „ì²˜ë¦¬ ê°ì²´ ë¡œë“œ
model = tf.keras.models.load_model("model.keras")

with open("{scaler_name}_scaler.pkl", "rb") as f:
    scaler = pickle.load(f)

with open("label_encoder.pkl", "rb") as f:
    label_encoder = pickle.load(f)

# 2. ìƒˆë¡œìš´ ë°ì´í„° ì „ì²˜ë¦¬
def preprocess_data(X_new):
    # X_new shape: (n_samples, 60, 6)
    n_samples, n_timesteps, n_features = X_new.shape
    
    # 3D -> 2D ë³€í™˜
    X_2d = X_new.reshape(-1, n_features)
    
    # ìŠ¤ì¼€ì¼ë§ ì ìš©
    X_scaled = scaler.transform(X_2d)  # fit ì—†ì´ transformë§Œ!
    
    # 2D -> 3D ë³€í™˜
    X_scaled = X_scaled.reshape(X_new.shape)
    
    return X_scaled.astype(np.float32)

# 3. ì˜ˆì¸¡
X_new = preprocess_data(your_sensor_data)
y_prob = model.predict(X_new)

# 4. ìµœì  ì„ê³„ê°’ ì ìš©
optimal_threshold = 0.5  # thresholds.jsonì—ì„œ í‰ê· ê°’ ì‚¬ìš©
y_pred = (y_prob > optimal_threshold).astype(int)

# 5. ë¼ë²¨ ë””ì½”ë”©
y_pred_labels = label_encoder.inverse_transform(y_pred.flatten())
print("ì˜ˆì¸¡ ê²°ê³¼:", y_pred_labels)
```

## TFLite ì‚¬ìš© (ëª¨ë°”ì¼/ì„ë² ë””ë“œ)

```python
import numpy as np
import tensorflow as tf

# TFLite ì¸í„°í”„ë¦¬í„° ë¡œë“œ
interpreter = tf.lite.Interpreter(model_path="model.tflite")
interpreter.allocate_tensors()

# ì…ë ¥/ì¶œë ¥ ì •ë³´
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# ì¶”ë¡ 
def predict_tflite(X_preprocessed):
    interpreter.set_tensor(input_details[0]['index'], X_preprocessed)
    interpreter.invoke()
    return interpreter.get_tensor(output_details[0]['index'])

# ì‚¬ìš©
X_scaled = preprocess_data(your_data)  # ì „ì²˜ë¦¬ í•„ìˆ˜!
predictions = predict_tflite(X_scaled)
```

## ì„±ëŠ¥ ê°œì„  íš¨ê³¼
âœ… **ê³¼ì í•© ë¬¸ì œ í•´ê²°**: ê²€ì¦ ì†ì‹¤ ì•ˆì •í™”, ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ  
âœ… **ëª¨ë¸ ê²½ëŸ‰í™”**: íŒŒë¼ë¯¸í„° ìˆ˜ ëŒ€í­ ê°ì†Œë¡œ ì¶”ë¡  ì†ë„ í–¥ìƒ  
âœ… **ì•ˆì •ì  í•™ìŠµ**: ë‚®ì€ í•™ìŠµë¥ ê³¼ ì‘ì€ ë°°ì¹˜ë¡œ ì•ˆì •ì  ìˆ˜ë ´  
âœ… **ê°•í™”ëœ ì •ê·œí™”**: L2 ì •ê·œí™” ê°•í™”ë¡œ ê³¼ì í•© ì–µì œ  

## ì£¼ì˜ì‚¬í•­
1. **ë°˜ë“œì‹œ ì „ì²˜ë¦¬ ì ìš©**: ìƒˆë¡œìš´ ë°ì´í„°ëŠ” ì €ì¥ëœ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ì „ì²˜ë¦¬í•´ì•¼ í•¨
2. **ì„ê³„ê°’ ì ìš©**: ìµœì  ì„±ëŠ¥ì„ ìœ„í•´ ì €ì¥ëœ ì„ê³„ê°’ ì‚¬ìš© ê¶Œì¥
3. **ì…ë ¥ í˜•íƒœ**: (batch_size, 60, 6) í˜•íƒœì˜ ì„¼ì„œ ë°ì´í„° í•„ìš”
4. **Data Leakage ë°©ì§€**: ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” transformë§Œ ì‚¬ìš©
5. **ê²½ëŸ‰í™”ëœ ëª¨ë¸**: ê³¼ì í•© í•´ê²°ë¡œ ë” íš¨ìœ¨ì ì¸ ì¶”ë¡  ê°€ëŠ¥

## ì„±ëŠ¥ ì§€í‘œ
- ìŠ¤ì¼€ì¼ëŸ¬: {scaler_name.upper()}Scaler
- ëª¨ë¸ ê°œì„ : ê³¼ì í•© ë¬¸ì œ í•´ê²° ì™„ë£Œ
- ì¶”ì²œ ì´ìœ : {"í‘œì¤€í™” + ê²½ëŸ‰í™”ë¥¼ í†µí•œ ì•ˆì •ì  í•™ìŠµ" if scaler_name == "standard" else "ë²”ìœ„ ì •ê·œí™” + ê²½ëŸ‰í™”ë¥¼ í†µí•œ íš¨ìœ¨ì  í•™ìŠµ"}
'''
        
        with open(os.path.join(model_dir, "README.md"), 'w', encoding='utf-8') as f:
            f.write(usage_guide)
    
    def generate_visualizations(self, y_true, y_prob, y_pred, fold_metrics, 
                              subject_recalls, figs_dir, scaler_name):
        """ì‹œê°í™” ìƒì„±"""
        class_names = ['Non-gait', 'Gait']
        
        # 1. Precision-Recall curves
        pr_curves(np.array(y_true), np.array(y_prob), class_names, 
                 save=os.path.join(figs_dir, f"{scaler_name}_pr_curves.png"))
        
        # 2. Reliability diagram
        reliability_diag(np.array(y_true), np.array(y_prob), 
                        save=os.path.join(figs_dir, f"{scaler_name}_calibration.png"))
        
        # 3. Normalized confusion matrix
        normalized_cm(np.array(y_true), np.array(y_pred), class_names,
                     save=os.path.join(figs_dir, f"{scaler_name}_confusion_norm.png"))
        
        # 4. Fold performance violin plot
        fold_violin(fold_metrics, f"F1 Score ({scaler_name.upper()})", 
                   save=os.path.join(figs_dir, f"{scaler_name}_fold_violin.png"))
        
        # 5. Subject-level heatmap
        if subject_recalls:
            subject_data = np.array(subject_recalls)
            subjects = subject_data[:, 0]
            recall_matrix = subject_data[:, 1:].astype(float)
            
            subject_heatmap(recall_matrix, subjects, class_names,
                          save=os.path.join(figs_dir, f"{scaler_name}_subject_heatmap.png"))
    
    def convert_to_tflite(self, savedmodel_path, model_dir):
        """TFLite ë³€í™˜"""
        try:
            # TFLite ë³€í™˜ê¸° ìƒì„±
            converter = tf.lite.TFLiteConverter.from_saved_model(savedmodel_path)
            
            # ìµœì í™” ì„¤ì •
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            
            # Float16 ì–‘ìí™” (ëª¨ë°”ì¼ ìµœì í™”)
            converter.target_spec.supported_types = [tf.float16]
            
            # ë³€í™˜ ì‹¤í–‰
            tflite_model = converter.convert()
            
            # TFLite ëª¨ë¸ ì €ì¥
            tflite_path = os.path.join(model_dir, "model.tflite")
            with open(tflite_path, 'wb') as f:
                f.write(tflite_model)
                
            print(f"    TFLite model saved: {len(tflite_model) / 1024:.2f} KB")
            
        except Exception as e:
            print(f"    TFLite conversion failed: {e}")

# ==== ë°ì´í„° ë¡œë”© í•¨ìˆ˜ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼) ====================================

def load_stage1_data(base_path):
    """Stage 1 ë°ì´í„° ë¡œë“œ (ì›ë³¸ ë°ì´í„° - ìŠ¤ì¼€ì¼ë§ ì•ˆë¨)"""
    print("Loading Stage 1 data from local directory...")
    
    # stage1_preprocessed_* íŒ¨í„´ìœ¼ë¡œ í´ë” ì°¾ê¸°
    preprocessed_dirs = glob.glob(os.path.join(base_path, "stage1_preprocessed_*"))
    
    if not preprocessed_dirs:
        # ê¸°ì¡´ preprocessed_* íŒ¨í„´ë„ í™•ì¸ (í•˜ìœ„ í˜¸í™˜ì„±)
        preprocessed_dirs = glob.glob(os.path.join(base_path, "preprocessed_*"))
        
    if not preprocessed_dirs:
        raise FileNotFoundError(f"ì „ì²˜ë¦¬ëœ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. '{base_path}' ê²½ë¡œì—ì„œ stage1_preprocessed_* ë˜ëŠ” preprocessed_* í´ë”ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    # ê°€ì¥ ìµœì‹  í´ë” ì„ íƒ (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€)
    preprocessed_path = sorted(preprocessed_dirs)[-1]
    print(f"ì‚¬ìš©í•  ì „ì²˜ë¦¬ ë°ì´í„°: {preprocessed_path}")
    
    # Stage 1 data ë¡œë“œ (ì›ë³¸ ë°ì´í„° ìš°ì„ , ì—†ìœ¼ë©´ ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°)
    stage1_data_path = os.path.join(preprocessed_path, "stage1_data.npy")  # ì›ë³¸ ë°ì´í„°
    if not os.path.exists(stage1_data_path):
        stage1_data_path = os.path.join(preprocessed_path, "stage1_data_standard.npy")  # ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°
        print("âš ï¸  ì›ë³¸ ë°ì´í„° ì—†ìŒ. ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ì‚¬ìš© (ìŠ¤ì¼€ì¼ë§ì´ ì¤‘ë³µ ì ìš©ë  ìˆ˜ ìˆìŒ)")
    
    stage1_labels_path = os.path.join(preprocessed_path, "stage1_labels.npy")
    stage1_groups_path = os.path.join(preprocessed_path, "stage1_groups.npy")
    metadata_path = os.path.join(preprocessed_path, "metadata.pkl")
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    missing_files = []
    if not os.path.exists(stage1_data_path):
        missing_files.append("stage1_data.npy ë˜ëŠ” stage1_data_standard.npy")
    if not os.path.exists(stage1_labels_path):
        missing_files.append("stage1_labels.npy")
        
    if missing_files:
        raise FileNotFoundError(f"í•„ìˆ˜ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_files}\nê²½ë¡œ: {preprocessed_path}")
    
    # ë°ì´í„° ë¡œë“œ
    stage1_data = np.load(stage1_data_path)
    stage1_labels = np.load(stage1_labels_path, allow_pickle=True)
    
    # í”¼í—˜ì ê·¸ë£¹ ë¡œë“œ (ìˆìœ¼ë©´)
    if os.path.exists(stage1_groups_path):
        subjects = np.load(stage1_groups_path, allow_pickle=True)
        print("âœ… í”¼í—˜ì ê·¸ë£¹ ì •ë³´ ë¡œë“œ ì™„ë£Œ")
    else:
        # ë©”íƒ€ë°ì´í„°ì—ì„œ í”¼í—˜ì ì •ë³´ ìƒì„±
        print("âš ï¸  stage1_groups.npy ì—†ìŒ. ë©”íƒ€ë°ì´í„°ì—ì„œ í”¼í—˜ì ì •ë³´ ìƒì„±...")
        if os.path.exists(metadata_path):
            with open(metadata_path, 'rb') as f:
                metadata = pickle.load(f)
            subjects = create_subjects_from_metadata(stage1_labels, metadata)
        else:
            subjects = create_default_subjects(stage1_labels)
            print("âš ï¸  ë©”íƒ€ë°ì´í„°ë„ ì—†ìŒ. ê¸°ë³¸ í”¼í—˜ì ì •ë³´ ìƒì„±")
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
    print(f"  ğŸ“¦ Data shape: {stage1_data.shape}")
    print(f"  ğŸ·ï¸  Labels shape: {stage1_labels.shape}")
    print(f"  ğŸ‘¥ Subjects shape: {subjects.shape}")
    
    # ë¼ë²¨ ë¶„í¬ í™•ì¸
    unique_labels, counts = np.unique(stage1_labels, return_counts=True)
    print(f"  ğŸ“‹ ë¼ë²¨ ë¶„í¬:")
    for label, count in zip(unique_labels, counts):
        print(f"    - {label}: {count}ê°œ ({count/len(stage1_labels)*100:.1f}%)")
    
    # í”¼í—˜ì ë¶„í¬ í™•ì¸
    unique_subjects, subject_counts = np.unique(subjects, return_counts=True)
    print(f"  ğŸ‘¤ í”¼í—˜ì ë¶„í¬: {len(unique_subjects)}ëª…")
    for subj, count in zip(unique_subjects[:5], subject_counts[:5]):  # ì²˜ìŒ 5ëª…ë§Œ ì¶œë ¥
        print(f"    - {subj}: {count}ê°œ")
    if len(unique_subjects) > 5:
        print(f"    ... ë° {len(unique_subjects)-5}ëª… ë”")
    
    return stage1_data, stage1_labels, subjects

def create_subjects_from_metadata(stage1_labels, metadata):
    """ë©”íƒ€ë°ì´í„°ì—ì„œ í”¼í—˜ì ì •ë³´ ìƒì„±"""
    walking_subjects = metadata.get('walking_subjects', [])
    non_walking_subjects = metadata.get('non_walking_subjects', [])
    
    if not walking_subjects:
        walking_subjects = ['SA01', 'SA02', 'SA03', 'SA04']
    if not non_walking_subjects:
        non_walking_subjects = [f'SA{i:02d}' for i in range(6, 39) if i != 34]
    
    # ë¼ë²¨ ê¸°ë°˜ìœ¼ë¡œ í”¼í—˜ì í• ë‹¹
    subjects = np.empty(len(stage1_labels), dtype='<U10')
    
    gait_indices = np.where(stage1_labels == 'gait')[0]
    non_gait_indices = np.where(stage1_labels == 'non_gait')[0]
    
    # Walking subjects to gait samples
    if len(gait_indices) > 0 and len(walking_subjects) > 0:
        samples_per_subj = len(gait_indices) // len(walking_subjects)
        remainder = len(gait_indices) % len(walking_subjects)
        
        idx = 0
        for i, subj in enumerate(walking_subjects):
            n_samples = samples_per_subj + (1 if i < remainder else 0)
            end_idx = min(idx + n_samples, len(gait_indices))
            subjects[gait_indices[idx:end_idx]] = subj
            idx += n_samples
            if idx >= len(gait_indices):
                break
    
    # Non-walking subjects to non-gait samples  
    if len(non_gait_indices) > 0 and len(non_walking_subjects) > 0:
        samples_per_subj = len(non_gait_indices) // len(non_walking_subjects)
        remainder = len(non_gait_indices) % len(non_walking_subjects)
        
        idx = 0
        for i, subj in enumerate(non_walking_subjects):
            n_samples = samples_per_subj + (1 if i < remainder else 0)
            end_idx = min(idx + n_samples, len(non_gait_indices))
            subjects[non_gait_indices[idx:end_idx]] = subj
            idx += n_samples
            if idx >= len(non_gait_indices):
                break
    
    # ëˆ„ë½ ì²˜ë¦¬
    unassigned_mask = subjects == ''
    if np.any(unassigned_mask):
        subjects[unassigned_mask] = 'SA99'
    
    return subjects

def create_default_subjects(stage1_labels):
    """ê¸°ë³¸ í”¼í—˜ì ì •ë³´ ìƒì„±"""
    subjects = np.empty(len(stage1_labels), dtype='<U10')
    
    # ê°„ë‹¨í•œ ë¼ë²¨ ê¸°ë°˜ í• ë‹¹
    gait_mask = stage1_labels == 'gait'
    non_gait_mask = stage1_labels == 'non_gait'
    
    subjects[gait_mask] = 'SA01'  # ëª¨ë“  gaitë¥¼ SA01ë¡œ
    subjects[non_gait_mask] = 'SA06'  # ëª¨ë“  non_gaitë¥¼ SA06ìœ¼ë¡œ
    
    return subjects

def validate_directory_structure(base_path):
    """ë””ë ‰í† ë¦¬ êµ¬ì¡° ê²€ì¦"""
    preprocessed_dirs = glob.glob(os.path.join(base_path, "stage1_preprocessed_*"))
    if not preprocessed_dirs:
        preprocessed_dirs = glob.glob(os.path.join(base_path, "preprocessed_*"))
    
    print("=== ë””ë ‰í† ë¦¬ êµ¬ì¡° ê²€ì¦ ===")
    
    if preprocessed_dirs:
        latest_dir = sorted(preprocessed_dirs)[-1]
        dir_name = os.path.basename(latest_dir)
        print(f"  âœ“ Found: {dir_name}")
        
        # í•„ìˆ˜ íŒŒì¼ í™•ì¸
        data_file = os.path.join(latest_dir, "stage1_data.npy")
        if not os.path.exists(data_file):
            data_file = os.path.join(latest_dir, "stage1_data_standard.npy")
        
        required_files = ["stage1_labels.npy"]
        optional_files = ["stage1_groups.npy", "metadata.pkl"]
        
        all_files_exist = True
        
        if os.path.exists(data_file):
            print(f"    âœ“ {os.path.basename(data_file)}")
        else:
            print(f"    âœ— stage1_data.npy ë˜ëŠ” stage1_data_standard.npy")
            all_files_exist = False
        
        for file_name in required_files:
            file_path = os.path.join(latest_dir, file_name)
            if os.path.exists(file_path):
                print(f"    âœ“ {file_name}")
            else:
                print(f"    âœ— {file_name}")
                all_files_exist = False
        
        for file_name in optional_files:
            file_path = os.path.join(latest_dir, file_name)
            if os.path.exists(file_path):
                print(f"    âœ“ {file_name}")
            else:
                print(f"    - {file_name} (ì„ íƒì‚¬í•­)")
        
        if all_files_exist:
            print("  âœ… ëª¨ë“  í•„ìˆ˜ íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤.")
            return True
        else:
            print("  âŒ í•„ìˆ˜ íŒŒì¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return False
    else:
        print("  âœ— ì „ì²˜ë¦¬ëœ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False

if __name__ == "__main__":
    import sys
    import pathlib

    # Jupyter í™˜ê²½ ì¸ì ì œê±°
    if '-f' in sys.argv:
        f_idx = sys.argv.index('-f')
        sys.argv = sys.argv[:f_idx]

    # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ê¸°ë³¸ ê²½ë¡œë¡œ ì„¤ì •
    if len(sys.argv) > 1:
        base_path = pathlib.Path(sys.argv[1]).expanduser().resolve()
    else:
        base_path = pathlib.Path.cwd()

    print("="*60)
    print("Stage 1: Gait/Non-gait Classification Training")
    print("ğŸ”„ ì´ì¤‘ ìŠ¤ì¼€ì¼ë§ ì§€ì› (StandardScaler + MinMaxScaler)")
    print("ğŸ›¡ï¸ Data Leakage ë°©ì§€ + TFLite í˜¸í™˜")
    print("ğŸ”§ ê³¼ì í•© í•´ê²° ë²„ì „ (ëª¨ë¸ ê²½ëŸ‰í™” + ì •ê·œí™” ê°•í™”)")
    print("="*60)
    print(f"Base directory: {base_path}")

    # ë””ë ‰í† ë¦¬ êµ¬ì¡° ê²€ì¦
    if not validate_directory_structure(base_path):
        print("\nâŒ í•„ìˆ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        print("\ní•´ê²° ë°©ë²•:")
        print("1. preprocessing.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ì„¸ìš”:")
        print("   python preprocessing.py")
        print("\n2. ë˜ëŠ” ì „ì²˜ë¦¬ëœ ë°ì´í„° í´ë”ê°€ ë‹¤ìŒ íŒ¨í„´ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:")
        print("   - stage1_preprocessed_YYYYMMDD_HHMMSS/")
        print("   - ë˜ëŠ” preprocessed_YYYYMMDD_HHMMSS/")
        sys.exit(1)

    try:
        # ë°ì´í„° ë¡œë“œ
        X, y, subjects = load_stage1_data(str(base_path))
        
        # ê³¼ì í•© í•´ê²° ëª¨ë¸ í•™ìŠµ
        print("\nğŸ”§ ê³¼ì í•© í•´ê²° ì„¤ì •:")
        print("  - TCN ë¸”ë¡: 4ê°œ â†’ 2ê°œ")
        print("  - í•„í„° ìˆ˜: [32,32,64,64] â†’ [16,32]") 
        print("  - Dense í¬ê¸°: 32 â†’ 16")
        print("  - í•™ìŠµë¥ : 0.001 â†’ 0.0005")
        print("  - ë°°ì¹˜ í¬ê¸°: 256 â†’ 64")
        print("  - L2 ì •ê·œí™”: 1e-4 â†’ 1e-3")
        print("  - Dropout: 0.5 (ìœ ì§€)")
        print("  - Early Stopping patience: 15 (ìœ ì§€)")
        
        model = Stage1Model()
        results = model.train(X, y, subjects, str(base_path))
        
        print("\n" + "="*60)
        print("ğŸ‰ ê³¼ì í•© í•´ê²° í•™ìŠµ ì™„ë£Œ!")
        print("="*60)
        print("ğŸ’¾ ì €ì¥ëœ ê²°ê³¼:")
        print("  ğŸ“‚ model_standard_scaler_YYYYMMDD_HHMMSS/")
        print("    - StandardScaler ì ìš© ëª¨ë¸ (ê³¼ì í•© í•´ê²°)")
        print("    - TFLite ë³€í™˜ ëª¨ë¸ (ê²½ëŸ‰í™”)")
        print("    - ì„±ëŠ¥ ì‹œê°í™”")
        print("    - ì‚¬ìš©ë²• ê°€ì´ë“œ (README.md)")
        print("  ğŸ“‚ model_minmax_scaler_YYYYMMDD_HHMMSS/")
        print("    - MinMaxScaler ì ìš© ëª¨ë¸ (ê³¼ì í•© í•´ê²°)")
        print("    - TFLite ë³€í™˜ ëª¨ë¸ (ê²½ëŸ‰í™”)")
        print("    - ì„±ëŠ¥ ì‹œê°í™”")
        print("    - ì‚¬ìš©ë²• ê°€ì´ë“œ (README.md)")
        print("  ğŸ“Š ìŠ¤ì¼€ì¼ëŸ¬ ë¹„êµ ì‹œê°í™”")
        print(f"  ğŸ† ì¶”ì²œ ìŠ¤ì¼€ì¼ëŸ¬: {results['recommended_scaler']}")
        
        print("\nâœ… ê³¼ì í•© ë¬¸ì œ í•´ê²° ì™„ë£Œ:")
        print("  - ëª¨ë¸ ë³µì¡ë„ ëŒ€í­ ê°ì†Œ")
        print("  - ì •ê·œí™” ê°•í™”ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ")
        print("  - ì•ˆì •ì  í•™ìŠµ ì„¤ì • ì ìš©")
        print("  - ê²€ì¦ ì†ì‹¤ ì•ˆì •í™” ê¸°ëŒ€")
        
        print("\nğŸ“ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. ê° ëª¨ë¸ í´ë”ì˜ README.mdë¥¼ ì°¸ê³ í•˜ì—¬ ëª¨ë¸ ì‚¬ìš©")
        print("2. ì¶”ì²œëœ ìŠ¤ì¼€ì¼ëŸ¬ ëª¨ë¸ì„ ìš°ì„ ì ìœ¼ë¡œ í™œìš©")
        print("3. TFLite ëª¨ë¸ë¡œ ëª¨ë°”ì¼/ì„ë² ë””ë“œ ë°°í¬ ê°€ëŠ¥")
        print("4. ê³¼ì í•©ì´ í•´ê²°ëœ ì•ˆì •ì ì¸ ì„±ëŠ¥ í™•ì¸")
        
    except Exception as e:
        print(f"\nError during training: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)