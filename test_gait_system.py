#!/usr/bin/env python3
"""
ë³´í–‰ ê°ì§€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì „ì²˜ë¦¬ëœ ìŠ¤ì¼€ì¼ëŸ¬ì™€ TFLite ëª¨ë¸ì´ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import os
import sys
import numpy as np
import pickle
import json
from datetime import datetime

def test_file_existence():
    """í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ í™•ì¸"""
    print("ğŸ” íŒŒì¼ ì¡´ì¬ í™•ì¸ ì¤‘...")
    
    required_files = {
        "MinMax Scaler": "scalers/gait/minmax_scaler.pkl",
        "Standard Scaler": "scalers/gait/standard_scaler.pkl", 
        "Metadata": "scalers/gait/metadata.json",
        "TFLite Model": "models/gait_detect/results/gait_detection.tflite"
    }
    
    all_exist = True
    for name, path in required_files.items():
        if os.path.exists(path):
            file_size = os.path.getsize(path)
            print(f"âœ… {name}: {path} ({file_size:,} bytes)")
        else:
            print(f"âŒ {name}: {path} (íŒŒì¼ ì—†ìŒ)")
            all_exist = False
    
    return all_exist

def test_scalers():
    """ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸"""
    print("\nğŸ”§ ìŠ¤ì¼€ì¼ëŸ¬ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    try:
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        with open("scalers/gait/minmax_scaler.pkl", 'rb') as f:
            minmax_scaler = pickle.load(f)
        print("âœ… MinMaxScaler ë¡œë“œ ì„±ê³µ")
        
        with open("scalers/gait/standard_scaler.pkl", 'rb') as f:
            standard_scaler = pickle.load(f)
        print("âœ… StandardScaler ë¡œë“œ ì„±ê³µ")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (60í”„ë ˆì„, 6ì¶• ì„¼ì„œ)
        test_data = np.random.randn(60, 6) * 10  # ëœë¤ ì„¼ì„œ ë°ì´í„°
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±: {test_data.shape}")
        print(f"   ì›ë³¸ ë²”ìœ„: {test_data.min():.3f} ~ {test_data.max():.3f}")
        
        # ìŠ¤ì¼€ì¼ë§ ì ìš© (preprocessing.pyì™€ ë™ì¼í•œ ìˆœì„œ)
        data_reshaped = test_data.reshape(-1, 6)
        
        # MinMax ìŠ¤ì¼€ì¼ë§
        data_minmax = minmax_scaler.transform(data_reshaped)
        data_minmax = data_minmax.reshape(60, 6)
        print(f"   MinMax í›„: {data_minmax.min():.3f} ~ {data_minmax.max():.3f}")
        
        # Standard ìŠ¤ì¼€ì¼ë§
        data_minmax_reshaped = data_minmax.reshape(-1, 6)
        data_scaled = standard_scaler.transform(data_minmax_reshaped)
        data_scaled = data_scaled.reshape(60, 6)
        print(f"   Standard í›„: {data_scaled.min():.3f} ~ {data_scaled.max():.3f}")
        
        print("âœ… ìŠ¤ì¼€ì¼ëŸ¬ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return data_scaled
        
    except Exception as e:
        print(f"âŒ ìŠ¤ì¼€ì¼ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return None

def test_metadata():
    """ë©”íƒ€ë°ì´í„° ë¡œë“œ ë° í™•ì¸"""
    print("\nğŸ“‹ ë©”íƒ€ë°ì´í„° í™•ì¸ ì¤‘...")
    
    try:
        # JSON í˜•ì‹ ì‹œë„
        if os.path.exists("scalers/gait/metadata.json"):
            with open("scalers/gait/metadata.json", 'r') as f:
                metadata = json.load(f)
        else:
            # PKL í˜•ì‹ ì‹œë„
            with open("scalers/gait/metadata.pkl", 'rb') as f:
                metadata = pickle.load(f)
        
        print("âœ… ë©”íƒ€ë°ì´í„° ë¡œë“œ ì„±ê³µ")
        print(f"   íƒ€ì„ìŠ¤íƒ¬í”„: {metadata.get('timestamp', 'N/A')}")
        print(f"   Stage1 ë°ì´í„° í˜•íƒœ: {metadata.get('stage1_shape', 'N/A')}")
        print(f"   Walking í”¼í—˜ì ìˆ˜: {metadata.get('n_walking_subjects', 'N/A')}")
        print(f"   Non-walking í”¼í—˜ì ìˆ˜: {metadata.get('n_non_walking_subjects', 'N/A')}")
        
        if 'stage1_label_distribution' in metadata:
            label_dist = metadata['stage1_label_distribution']
            print(f"   ë¼ë²¨ ë¶„í¬: {label_dist}")
        
        return metadata
        
    except Exception as e:
        print(f"âŒ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def test_tflite_model(test_data):
    """TFLite ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¤– TFLite ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    try:
        # TensorFlow Lite ê°€ì ¸ì˜¤ê¸° ì‹œë„
        try:
            import tensorflow as tf
            print("âœ… TensorFlow ì‚¬ìš©")
        except ImportError:
            try:
                import tflite_runtime.interpreter as tflite
                tf = tflite
                print("âœ… TFLite Runtime ì‚¬ìš©")
            except ImportError:
                print("âŒ TensorFlow ë˜ëŠ” TFLite Runtimeì´ í•„ìš”í•©ë‹ˆë‹¤")
                return False
        
        # ëª¨ë¸ ë¡œë“œ
        model_path = "models/gait_detect/results/gait_detection.tflite"
        interpreter = tf.lite.Interpreter(model_path=model_path)
        interpreter.allocate_tensors()
        
        # ì…ì¶œë ¥ ì •ë³´ í™•ì¸
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        
        print(f"âœ… TFLite ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        print(f"   ì…ë ¥ í˜•íƒœ: {input_details[0]['shape']}")
        print(f"   ì…ë ¥ íƒ€ì…: {input_details[0]['dtype']}")
        print(f"   ì¶œë ¥ í˜•íƒœ: {output_details[0]['shape']}")
        print(f"   ì¶œë ¥ íƒ€ì…: {output_details[0]['dtype']}")
        
        # ì¶”ë¡  í…ŒìŠ¤íŠ¸
        if test_data is not None:
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€: (60, 6) â†’ (1, 60, 6)
            input_data = np.expand_dims(test_data, axis=0).astype(np.float32)
            
            # ì¶”ë¡  ìˆ˜í–‰
            interpreter.set_tensor(input_details[0]['index'], input_data)
            interpreter.invoke()
            
            # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            output_data = interpreter.get_tensor(output_details[0]['index'])
            prediction = float(output_data[0][0])
            
            print(f"âœ… ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            print(f"   ì˜ˆì¸¡ í™•ë¥ : {prediction:.6f}")
            print(f"   ì˜ˆì¸¡ ê²°ê³¼: {'ë³´í–‰' if prediction > 0.5 else 'ë¹„ë³´í–‰'}")
            
            return True
        else:
            print("âš ï¸  í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ì–´ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ìƒëµ")
            return True
            
    except Exception as e:
        print(f"âŒ TFLite ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_processing_pipeline():
    """ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ”„ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    try:
        # ì‹œë®¬ë ˆì´ì…˜ ì„¼ì„œ ë°ì´í„° ìƒì„± (ì‹¤ì œ ì„¼ì„œ ë°ì´í„°ì™€ ìœ ì‚¬)
        # ê°€ì†ë„: Â±2g, ìì´ë¡œ: Â±250deg/s ë²”ìœ„
        accel_data = np.random.uniform(-2, 2, (60, 3))  # ê°€ì†ë„ 3ì¶•
        gyro_data = np.random.uniform(-250, 250, (60, 3))  # ìì´ë¡œ 3ì¶•
        sensor_data = np.concatenate([accel_data, gyro_data], axis=1)
        
        print(f"ğŸ“Š ì‹œë®¬ë ˆì´ì…˜ ì„¼ì„œ ë°ì´í„°: {sensor_data.shape}")
        print(f"   ê°€ì†ë„ ë²”ìœ„: {accel_data.min():.3f} ~ {accel_data.max():.3f} m/sÂ²")
        print(f"   ìì´ë¡œ ë²”ìœ„: {gyro_data.min():.3f} ~ {gyro_data.max():.3f} deg/s")
        
        # 1ë‹¨ê³„: ìŠ¤ì¼€ì¼ë§
        with open("scalers/gait/minmax_scaler.pkl", 'rb') as f:
            minmax_scaler = pickle.load(f)
        with open("scalers/gait/standard_scaler.pkl", 'rb') as f:
            standard_scaler = pickle.load(f)
        
        data_reshaped = sensor_data.reshape(-1, 6)
        data_minmax = minmax_scaler.transform(data_reshaped)
        data_scaled = standard_scaler.transform(data_minmax)
        data_scaled = data_scaled.reshape(60, 6)
        
        # 2ë‹¨ê³„: TFLite ì¶”ë¡ 
        import tensorflow as tf
        interpreter = tf.lite.Interpreter("models/gait_detect/results/gait_detection.tflite")
        interpreter.allocate_tensors()
        
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        
        input_data = np.expand_dims(data_scaled, axis=0).astype(np.float32)
        interpreter.set_tensor(input_details[0]['index'], input_data)
        interpreter.invoke()
        
        output_data = interpreter.get_tensor(output_details[0]['index'])
        prediction = float(output_data[0][0])
        
        print(f"âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        print(f"   ìµœì¢… ì˜ˆì¸¡: {prediction:.6f} ({'ë³´í–‰' if prediction > 0.5 else 'ë¹„ë³´í–‰'})")
        
        return True
        
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def generate_summary_report():
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
    print("\n" + "="*60)
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("="*60)
    
    # í˜„ì¬ ì‹œê°„
    print(f"â° í…ŒìŠ¤íŠ¸ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # í™˜ê²½ ì •ë³´
    print(f"ğŸ Python ë²„ì „: {sys.version.split()[0]}")
    
    # íŒ¨í‚¤ì§€ ë²„ì „ í™•ì¸
    try:
        import numpy
        print(f"ğŸ“Š NumPy: {numpy.__version__}")
    except:
        print("ğŸ“Š NumPy: ë¯¸ì„¤ì¹˜")
    
    try:
        import scipy
        print(f"ğŸ”¬ SciPy: {scipy.__version__}")
    except:
        print("ğŸ”¬ SciPy: ë¯¸ì„¤ì¹˜")
    
    try:
        import sklearn
        print(f"ğŸ§  scikit-learn: {sklearn.__version__}")
    except:
        print("ğŸ§  scikit-learn: ë¯¸ì„¤ì¹˜")
    
    try:
        import tensorflow as tf
        print(f"ğŸ¤– TensorFlow: {tf.__version__}")
    except:
        try:
            import tflite_runtime
            print(f"ğŸ¤– TFLite Runtime: ì„¤ì¹˜ë¨")
        except:
            print("ğŸ¤– TensorFlow/TFLite: ë¯¸ì„¤ì¹˜")
    
    print("="*60)

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ ë³´í–‰ ê°ì§€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*60)
    
    test_results = []
    
    # 1. íŒŒì¼ ì¡´ì¬ í™•ì¸
    files_ok = test_file_existence()
    test_results.append(("íŒŒì¼ ì¡´ì¬ í™•ì¸", files_ok))
    
    if not files_ok:
        print("\nâŒ í•„ìˆ˜ íŒŒì¼ì´ ëˆ„ë½ë˜ì–´ í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return False
    
    # 2. ìŠ¤ì¼€ì¼ëŸ¬ í…ŒìŠ¤íŠ¸
    scaled_data = test_scalers()
    test_results.append(("ìŠ¤ì¼€ì¼ëŸ¬ í…ŒìŠ¤íŠ¸", scaled_data is not None))
    
    # 3. ë©”íƒ€ë°ì´í„° í…ŒìŠ¤íŠ¸
    metadata = test_metadata()
    test_results.append(("ë©”íƒ€ë°ì´í„° í…ŒìŠ¤íŠ¸", metadata is not None))
    
    # 4. TFLite ëª¨ë¸ í…ŒìŠ¤íŠ¸
    model_ok = test_tflite_model(scaled_data)
    test_results.append(("TFLite ëª¨ë¸ í…ŒìŠ¤íŠ¸", model_ok))
    
    # 5. ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
    if all([scaled_data is not None, model_ok]):
        pipeline_ok = test_processing_pipeline()
        test_results.append(("ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸", pipeline_ok))
    else:
        print("\nâš ï¸  ì´ì „ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ë¡œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ìƒëµ")
        test_results.append(("ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸", False))
    
    # ê²°ê³¼ ìš”ì•½
    generate_summary_report()
    
    print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    all_passed = True
    for test_name, result in test_results:
        status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨"
        print(f"   {test_name}: {status}")
        if not result:
            all_passed = False
    
    if all_passed:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µí–ˆìŠµë‹ˆë‹¤!")
        print("   ë¼ì¦ˆë² ë¦¬íŒŒì´ì—ì„œ ì‹¤ì‹œê°„ ë³´í–‰ ê°ì§€ë¥¼ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸  ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("   README_usage_guide.mdë¥¼ ì°¸ê³ í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”.")
    
    return all_passed

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 