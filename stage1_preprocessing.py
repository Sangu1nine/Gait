import os
import numpy as np
import pandas as pd
from scipy import signal
import pickle
from datetime import datetime
import glob
import json
import warnings

# ê²½ê³  í•„í„°ë§
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)

class GaitDataPreprocessor:
    def __init__(self, base_path="C:/Gait"):
        self.base_path = base_path
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ë²„í„°ì›ŒìŠ¤ í•„í„° ì„¤ì •
        self.fs = 30  # ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜ 30Hz
        self.cutoff = 10  # ì»·ì˜¤í”„ ì£¼íŒŒìˆ˜ 10Hz
        self.order = 4
        
        # ê²½ë¡œ í™•ì¸ ë° ì¶œë ¥
        print(f"ğŸ“ Base path: {self.base_path}")
        print(f"ğŸ“ Walking data path: {os.path.join(self.base_path, 'walking_data')}")
        print(f"ğŸ“ Non-walking data path: {os.path.join(self.base_path, 'Selected_non_30hz')}")
        print(f"ğŸ“ Label data path: {os.path.join(self.base_path, 'support_label_data')}")
        
    def butter_lowpass_filter(self, data):
        """ë²„í„°ì›ŒìŠ¤ ë¡œìš°íŒ¨ìŠ¤ í•„í„° ì ìš©"""
        try:
            nyq = 0.5 * self.fs
            normal_cutoff = self.cutoff / nyq
            b, a = signal.butter(self.order, normal_cutoff, btype='low', analog=False)
            filtered_data = signal.filtfilt(b, a, data, axis=0)
            return filtered_data.astype(np.float32)
        except Exception as e:
            print(f"    âš ï¸  í•„í„° ì ìš© ì˜¤ë¥˜: {str(e)}")
            return data.astype(np.float32)
    
    def extract_subject_from_filename(self, filepath):
        """íŒŒì¼ ê²½ë¡œì—ì„œ í”¼í—˜ì ID ì¶”ì¶œ"""
        # ê²½ë¡œì—ì„œ í”¼í—˜ì í´ë”ëª… ì¶”ì¶œ (ì˜ˆ: .../SA01/S1_L_01.csv -> SA01)
        path_parts = filepath.replace('\\', '/').split('/')
        for part in path_parts:
            if part.startswith('SA') and len(part) == 4:
                return part
        
        # í´ë”ëª…ì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš° íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ ì‹œë„
        filename = os.path.basename(filepath)
        if filename.startswith('SA'):
            return filename[:4]
        
        return None
    
    def load_walking_data(self):
        """walking_data ë¡œë“œ (ì‹¤ì œ íŒŒì¼ì—ì„œ í”¼í—˜ì ID ì¶”ì¶œ)"""
        walking_data = []
        walking_labels = []
        walking_subjects = []
        walking_filenames = []
        
        print("\nğŸš¶ Walking ë°ì´í„° ë¡œë”© ì¤‘...")
        
        # ëª¨ë“  walking_data í•˜ìœ„ í´ë” ê²€ìƒ‰
        walking_base = os.path.join(self.base_path, "walking_data")
        if not os.path.exists(walking_base):
            print(f"âŒ Walking data í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {walking_base}")
            return [], [], [], []
        
        # SA*ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  í´ë” ì°¾ê¸°
        subject_folders = glob.glob(os.path.join(walking_base, "SA*"))
        
        total_files = 0
        for subj_folder in sorted(subject_folders):
            subj_id = os.path.basename(subj_folder)
            print(f"  ğŸ“‚ {subj_id} ì²˜ë¦¬ ì¤‘...")
            
            csv_files = glob.glob(os.path.join(subj_folder, "*.csv"))
            file_count = 0
            
            for csv_file in csv_files:
                try:
                    # ì„¼ì„œ ë°ì´í„° ë¡œë“œ
                    df = pd.read_csv(csv_file)
                    sensor_cols = ['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z']
                    
                    # ì„¼ì„œ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
                    missing_cols = [col for col in sensor_cols if col not in df.columns]
                    if missing_cols:
                        print(f"    âš ï¸  {os.path.basename(csv_file)}: ì„¼ì„œ ì»¬ëŸ¼ ëˆ„ë½ {missing_cols}")
                        continue
                    
                    sensor_data = df[sensor_cols].values.astype(np.float32)
                    
                    # ë¼ë²¨ ë°ì´í„° ë¡œë“œ
                    filename = os.path.basename(csv_file).replace('.csv', '')
                    label_file = os.path.join(self.base_path, f"support_label_data/{subj_id}/{filename}_support_labels.csv")
                    
                    if os.path.exists(label_file):
                        label_df = pd.read_csv(label_file)
                        
                        # í”„ë ˆì„ë³„ ë¼ë²¨ ìƒì„±
                        frame_labels = np.full(len(sensor_data), 'non_gait', dtype='U20')
                        
                        for _, row in label_df.iterrows():
                            start = max(0, int(row['start_frame']) - 1)  # 0-indexed, ìŒìˆ˜ ë°©ì§€
                            end = min(len(sensor_data), int(row['end_frame']))  # ë²”ìœ„ ì´ˆê³¼ ë°©ì§€
                            if start < end:
                                frame_labels[start:end] = row['phase']
                        
                        # ì‹¤ì œ íŒŒì¼ ê²½ë¡œì—ì„œ í”¼í—˜ì ID ì¶”ì¶œ
                        extracted_subject = self.extract_subject_from_filename(csv_file)
                        final_subject_id = extracted_subject if extracted_subject else subj_id
                        
                        walking_data.append(sensor_data)
                        walking_labels.append(frame_labels)
                        walking_subjects.append(final_subject_id)
                        walking_filenames.append(os.path.basename(csv_file))
                        file_count += 1
                        
                    else:
                        print(f"    âš ï¸  ë¼ë²¨ íŒŒì¼ ì—†ìŒ: {os.path.basename(label_file)}")
                        
                except Exception as e:
                    print(f"    âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ {os.path.basename(csv_file)}: {str(e)}")
                    
            if file_count > 0:
                print(f"    âœ… {subj_id}: {file_count}ê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
                total_files += file_count
                    
        print(f"ğŸš¶ ì´ {total_files}ê°œ walking íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
        unique_subjects = list(set(walking_subjects))
        print(f"ğŸ“Š Walking í”¼í—˜ì: {sorted(unique_subjects)}")
        
        return walking_data, walking_labels, walking_subjects, walking_filenames
    
    def load_non_walking_data(self):
        """non-walking ë°ì´í„° ë¡œë“œ (ì‹¤ì œ íŒŒì¼ì—ì„œ í”¼í—˜ì ID ì¶”ì¶œ)"""
        non_walking_data = []
        non_walking_subjects = []
        non_walking_filenames = []
        
        print("\nğŸƒ Non-walking ë°ì´í„° ë¡œë”© ì¤‘...")
        
        # ëª¨ë“  Selected_non_30hz í•˜ìœ„ í´ë” ê²€ìƒ‰
        non_walking_base = os.path.join(self.base_path, "Selected_non_30hz")
        if not os.path.exists(non_walking_base):
            print(f"âŒ Non-walking data í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {non_walking_base}")
            return [], [], []
        
        # SA*ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  í´ë” ì°¾ê¸° (SA34 ì œì™¸)
        subject_folders = glob.glob(os.path.join(non_walking_base, "SA*"))
        subject_folders = [f for f in subject_folders if not os.path.basename(f) == 'SA34']
        
        total_files = 0
        for subj_folder in sorted(subject_folders):
            subj_id = os.path.basename(subj_folder)
            
            csv_files = glob.glob(os.path.join(subj_folder, "*.csv"))
            file_count = 0
            
            for csv_file in csv_files:
                try:
                    df = pd.read_csv(csv_file)
                    sensor_cols = ['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z']
                    
                    # ì„¼ì„œ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
                    missing_cols = [col for col in sensor_cols if col not in df.columns]
                    if missing_cols:
                        print(f"    âš ï¸  {os.path.basename(csv_file)}: ì„¼ì„œ ì»¬ëŸ¼ ëˆ„ë½ {missing_cols}")
                        continue
                        
                    sensor_data = df[sensor_cols].values.astype(np.float32)
                    
                    # ì‹¤ì œ íŒŒì¼ ê²½ë¡œì—ì„œ í”¼í—˜ì ID ì¶”ì¶œ
                    extracted_subject = self.extract_subject_from_filename(csv_file)
                    final_subject_id = extracted_subject if extracted_subject else subj_id
                    
                    non_walking_data.append(sensor_data)
                    non_walking_subjects.append(final_subject_id)
                    non_walking_filenames.append(os.path.basename(csv_file))
                    file_count += 1
                    
                except Exception as e:
                    print(f"    âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ {os.path.basename(csv_file)}: {str(e)}")
                    
            if file_count > 0:
                print(f"  âœ… {subj_id}: {file_count}ê°œ íŒŒì¼ ë¡œë“œ")
                total_files += file_count
                
        print(f"ğŸƒ ì´ {total_files}ê°œ non-walking íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
        unique_subjects = list(set(non_walking_subjects))
        print(f"ğŸ“Š Non-walking í”¼í—˜ì: {sorted(unique_subjects)}")
        
        return non_walking_data, non_walking_subjects, non_walking_filenames
    
    def apply_filtering(self, data_list):
        """ëª¨ë“  ë°ì´í„°ì— ë²„í„°ì›ŒìŠ¤ í•„í„° ì ìš©"""
        if not data_list:
            print("âš ï¸  í•„í„°ë§í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
            
        filtered_data = []
        for i, data in enumerate(data_list):
            if i % 100 == 0:  # ì§„í–‰ë¥  í‘œì‹œ
                print(f"  ğŸ“Š í•„í„°ë§ ì§„í–‰ë¥ : {i+1}/{len(data_list)}")
                
            filtered = np.zeros_like(data, dtype=np.float32)
            for j in range(data.shape[1]):  # ê° ì±„ë„ë³„ë¡œ í•„í„° ì ìš©
                filtered[:, j] = self.butter_lowpass_filter(data[:, j])
            filtered_data.append(filtered)
        return filtered_data
    
    def create_stage1_windows(self, data_list, subjects_list, filenames_list, labels_list=None):
        """Stage1ìš© ìœˆë„ìš° ìƒì„± (ì •í™•í•œ í”¼í—˜ì ë§¤í•‘)"""
        window_size = 60
        stride = 30
        
        windows = []
        window_labels = []
        window_subjects = []
        window_file_info = []  # ì–´ë–¤ íŒŒì¼ì—ì„œ ì™”ëŠ”ì§€ ì¶”ì 
        
        if not data_list:
            print("âš ï¸  ìœˆë„ìš° ìƒì„±í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return np.array([]).reshape(0, window_size, 6), np.array([]), np.array([]), []
        
        for idx, data in enumerate(data_list):
            subject_id = subjects_list[idx]
            filename = filenames_list[idx]
            n_frames = len(data)
            
            for start in range(0, n_frames - window_size + 1, stride):
                end = start + window_size
                windows.append(data[start:end])
                window_subjects.append(subject_id)
                window_file_info.append({
                    'subject': subject_id,
                    'filename': filename,
                    'start_frame': start,
                    'end_frame': end
                })
                
                if labels_list is not None:
                    # Walking ë°ì´í„°: gait phase ê¸°ë°˜ ë¼ë²¨ë§
                    window_label_seq = labels_list[idx][start:end]
                    
                    # Gait phaseë“¤ì„ gaitë¡œ ë³€í™˜
                    gait_phases = ['DS', 'SSR', 'SSL', 'double_support', 
                                 'single_support_left', 'single_support_right']
                    binary_labels = ['gait' if l in gait_phases else 'non_gait' 
                                   for l in window_label_seq]
                    
                    # ìœˆë„ìš°ì˜ ëŒ€í‘œ ë¼ë²¨ (50% ì´ìƒì¸ í´ë˜ìŠ¤)
                    gait_ratio = np.mean([1 if l == 'gait' else 0 for l in binary_labels])
                    majority_label = 'gait' if gait_ratio >= 0.5 else 'non_gait'
                    window_labels.append(majority_label)
                else:
                    # Non-walking ë°ì´í„°ëŠ” ëª¨ë‘ non_gait
                    window_labels.append('non_gait')
                    
        if windows:
            return (np.array(windows, dtype=np.float32), 
                   np.array(window_labels, dtype='U20'),
                   np.array(window_subjects, dtype='U20'),
                   window_file_info)
        else:
            return (np.array([]).reshape(0, window_size, 6), 
                   np.array([]), 
                   np.array([]),
                   [])
    
    def process_and_save(self):
        """ì „ì²´ ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (Data Leakage í•´ê²°)"""
        print("=" * 60)
        print("ğŸš€ Stage1 ë°ì´í„° ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ (Data Leakage ë°©ì§€)")
        print("=" * 60)
        
        # 1. ë°ì´í„° ë¡œë“œ
        walking_data, walking_labels, walking_subjects, walking_files = self.load_walking_data()
        non_walking_data, non_walking_subjects, non_walking_files = self.load_non_walking_data()
        
        print(f"\nğŸ“Š ë°ì´í„° ë¡œë“œ ê²°ê³¼:")
        print(f"  ğŸš¶ Walking ë°ì´í„°: {len(walking_data)}ê°œ íŒŒì¼")
        print(f"  ğŸƒ Non-walking ë°ì´í„°: {len(non_walking_data)}ê°œ íŒŒì¼")
        
        # ë°ì´í„°ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì˜¤ë¥˜
        if len(walking_data) == 0 and len(non_walking_data) == 0:
            raise ValueError("âŒ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œì™€ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        # 2. ë²„í„°ì›ŒìŠ¤ í•„í„° ì ìš©
        print(f"\nğŸ”§ ë²„í„°ì›ŒìŠ¤ í•„í„° ì ìš© ì¤‘...")
        if walking_data:
            walking_data = self.apply_filtering(walking_data)
            print(f"  âœ… Walking ë°ì´í„° í•„í„°ë§ ì™„ë£Œ")
        if non_walking_data:
            non_walking_data = self.apply_filtering(non_walking_data)
            print(f"  âœ… Non-walking ë°ì´í„° í•„í„°ë§ ì™„ë£Œ")
        
        # 3. Stage1 ìœˆë„ìš° ìƒì„± (ì •í™•í•œ í”¼í—˜ì ë§¤í•‘)
        print(f"\nğŸ“¦ Stage1 ìœˆë„ìš° ìƒì„± ì¤‘...")
        walking_windows, walking_labels_w, walking_groups, walking_info = self.create_stage1_windows(
            walking_data, walking_subjects, walking_files, walking_labels)
        non_walking_windows, non_walking_labels_w, non_walking_groups, non_walking_info = self.create_stage1_windows(
            non_walking_data, non_walking_subjects, non_walking_files)
        
        print(f"  ğŸš¶ Walking windows: {walking_windows.shape}")
        print(f"  ğŸƒ Non-walking windows: {non_walking_windows.shape}")
        
        # ì „ì²´ Stage1 ë°ì´í„° ê²°í•©
        if walking_windows.size > 0 and non_walking_windows.size > 0:
            all_windows = np.vstack([walking_windows, non_walking_windows])
            all_labels = np.hstack([walking_labels_w, non_walking_labels_w])
            all_groups = np.hstack([walking_groups, non_walking_groups])
            all_info = walking_info + non_walking_info
        elif walking_windows.size > 0:
            all_windows = walking_windows
            all_labels = walking_labels_w
            all_groups = walking_groups
            all_info = walking_info
        elif non_walking_windows.size > 0:
            all_windows = non_walking_windows
            all_labels = non_walking_labels_w
            all_groups = non_walking_groups
            all_info = non_walking_info
        else:
            raise ValueError("âŒ ìƒì„±ëœ ìœˆë„ìš°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"  ğŸ“¦ ì „ì²´ Stage1 windows: {all_windows.shape}")
        
        # í”¼í—˜ì ì •ë³´ ê²€ì¦ ë° ì¶œë ¥
        unique_subjects = np.unique(all_groups)
        print(f"  ğŸ‘¥ ì´ í”¼í—˜ì ìˆ˜: {len(unique_subjects)}ëª…")
        print(f"  ğŸ‘¥ í”¼í—˜ì ëª©ë¡: {sorted(unique_subjects)}")
        
        # í”¼í—˜ìë³„ ìƒ˜í”Œ ìˆ˜ í™•ì¸
        subject_counts = {}
        for subject in unique_subjects:
            count = np.sum(all_groups == subject)
            subject_counts[subject] = int(count)
        
        print(f"  ğŸ“Š í”¼í—˜ìë³„ ìœˆë„ìš° ìˆ˜:")
        for subject in sorted(subject_counts.keys()):
            print(f"    - {subject}: {subject_counts[subject]}ê°œ")
        
        # 4. ì €ì¥ (ìŠ¤ì¼€ì¼ë§ ì œê±° - ëª¨ë¸ì—ì„œ ì²˜ë¦¬)
        save_dir = os.path.join(self.base_path, f"stage1_preprocessed_{self.timestamp}")
        os.makedirs(save_dir, exist_ok=True)
        
        print(f"\nğŸ’¾ Stage1 ë°ì´í„° ì €ì¥ ì¤‘: {save_dir}")
        
        # ì›ë³¸ ë°ì´í„°ë§Œ ì €ì¥ (ìŠ¤ì¼€ì¼ë§ì€ ëª¨ë¸ì—ì„œ ìˆ˜í–‰)
        np.save(os.path.join(save_dir, "stage1_data.npy"), all_windows)
        np.save(os.path.join(save_dir, "stage1_labels.npy"), all_labels)
        np.save(os.path.join(save_dir, "stage1_groups.npy"), all_groups)
        
        # ìƒì„¸ ì •ë³´ ì €ì¥
        with open(os.path.join(save_dir, "window_info.pkl"), 'wb') as f:
            pickle.dump(all_info, f)
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        walking_subjects_unique = list(set(walking_subjects)) if walking_subjects else []
        non_walking_subjects_unique = list(set(non_walking_subjects)) if non_walking_subjects else []
        
        metadata = {
            'timestamp': self.timestamp,
            'processing_stage': 'stage1_only',
            'data_leakage_prevention': True,
            'scaling_applied': False,
            'scaling_note': 'Scaling should be applied in model training using GroupKFold',
            'n_walking_subjects': len(walking_subjects_unique),
            'n_non_walking_subjects': len(non_walking_subjects_unique),
            'stage1_shape': list(all_windows.shape),
            'walking_subjects': walking_subjects_unique,
            'non_walking_subjects': non_walking_subjects_unique,
            'all_subjects': list(unique_subjects),
            'stage1_label_distribution': {
                str(label): int(count) for label, count in 
                zip(*np.unique(all_labels, return_counts=True))
            },
            'subject_window_counts': subject_counts,
            'total_files_processed': {
                'walking': len(walking_data),
                'non_walking': len(non_walking_data)
            },
            'window_config': {
                'window_size': 60,
                'stride': 30,
                'sampling_rate': 30,
                'label_threshold': 0.5  # 50% ì´ìƒ gaitë©´ gait ë¼ë²¨
            },
            'filter_config': {
                'type': 'butterworth_lowpass',
                'order': 4,
                'cutoff_hz': 10,
                'sampling_rate': 30
            }
        }
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(os.path.join(save_dir, "metadata.pkl"), 'wb') as f:
            pickle.dump(metadata, f)
        
        with open(os.path.join(save_dir, "metadata.json"), 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
            
        # GroupKFold ì‚¬ìš© ì˜ˆì‹œ ì €ì¥ (ìŠ¤ì¼€ì¼ë§ í¬í•¨)
        example_code = '''# Data Leakage ë°©ì§€ë¥¼ ìœ„í•œ ì˜¬ë°”ë¥¸ ì‚¬ìš©ë²•
import numpy as np
from sklearn.model_selection import GroupKFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.pipeline import Pipeline

# ë°ì´í„° ë¡œë“œ
X = np.load("stage1_data.npy")  # ì›ë³¸ ë°ì´í„° (ìŠ¤ì¼€ì¼ë§ ì•ˆë¨)
y = np.load("stage1_labels.npy")
groups = np.load("stage1_groups.npy")

print(f"ë°ì´í„° í˜•íƒœ: {X.shape}")
print(f"ë¼ë²¨ ìˆ˜: {len(np.unique(y))}")
print(f"í”¼í—˜ì ìˆ˜: {len(np.unique(groups))}")

# ì˜¬ë°”ë¥¸ ë°©ë²•: GroupKFold + ë‚´ë¶€ ìŠ¤ì¼€ì¼ë§
from sklearn.preprocessing import LabelEncoder

# ë¼ë²¨ ì¸ì½”ë”©
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# GroupKFold ì„¤ì •
gkf = GroupKFold(n_splits=5)

for fold, (train_idx, test_idx) in enumerate(gkf.split(X, y_encoded, groups)):
    # ë°ì´í„° ë¶„í• 
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]
    
    # í›ˆë ¨ ì„¸íŠ¸ë¡œë§Œ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
    scaler = StandardScaler()
    
    # 3D ë°ì´í„°ë¥¼ 2Dë¡œ ë³€í™˜í•˜ì—¬ ìŠ¤ì¼€ì¼ë§
    n_samples, n_timesteps, n_features = X_train.shape
    X_train_2d = X_train.reshape(-1, n_features)
    X_test_2d = X_test.reshape(-1, n_features)
    
    # í›ˆë ¨ ë°ì´í„°ë¡œ fit, í›ˆë ¨+í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— transform
    X_train_scaled = scaler.fit_transform(X_train_2d)
    X_test_scaled = scaler.transform(X_test_2d)  # fit ì—†ì´ transformë§Œ!
    
    # ë‹¤ì‹œ 3Dë¡œ ë³€í™˜
    X_train_scaled = X_train_scaled.reshape(X_train.shape)
    X_test_scaled = X_test_scaled.reshape(X_test.shape)
    
    # í”¼í—˜ì í™•ì¸
    train_subjects = np.unique(groups[train_idx])
    test_subjects = np.unique(groups[test_idx])
    
    print(f"Fold {fold+1}:")
    print(f"  Train í”¼í—˜ì: {train_subjects}")
    print(f"  Test í”¼í—˜ì: {test_subjects}")
    print(f"  Train ìƒ˜í”Œ: {len(X_train_scaled)}, Test ìƒ˜í”Œ: {len(X_test_scaled)}")
    
    # ì—¬ê¸°ì„œ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    # model.fit(X_train_scaled, y_train)
    # score = model.evaluate(X_test_scaled, y_test)

# ì˜ëª»ëœ ë°©ë²• (í•˜ì§€ ë§ˆì„¸ìš”!)
# scaler = StandardScaler()
# X_all_scaled = scaler.fit_transform(X.reshape(-1, X.shape[-1]))  # ì „ì²´ ë°ì´í„°ë¡œ fit!
# ì´ë ‡ê²Œ í•˜ë©´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë³´ê°€ í›ˆë ¨ì— ëˆ„ì¶œë©ë‹ˆë‹¤.
'''
        
        with open(os.path.join(save_dir, "correct_usage_example.py"), 'w', encoding='utf-8') as f:
            f.write(example_code)
            
        print("=" * 60)
        print("âœ… Stage1 ì „ì²˜ë¦¬ ì™„ë£Œ! (Data Leakage ë°©ì§€)")
        print("=" * 60)
        print(f"ğŸ“Š ìµœì¢… ê²°ê³¼:")
        print(f"  ğŸ“¦ Stage1 ë°ì´í„°: {all_windows.shape} (ìŠ¤ì¼€ì¼ë§ ì•ˆë¨)")
        print(f"  ğŸ“‹ ë¼ë²¨ ë¶„í¬: {metadata['stage1_label_distribution']}")
        print(f"  ğŸ‘¥ ì „ì²´ í”¼í—˜ì: {len(unique_subjects)}ëª…")
        print(f"  ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {save_dir}")
        print(f"  ğŸ›¡ï¸  Data Leakage ë°©ì§€: âœ…")
        print(f"  ğŸ“ ì˜¬ë°”ë¥¸ ì‚¬ìš©ë²•: correct_usage_example.py ì°¸ì¡°")
        print("=" * 60)
        print("âš ï¸  ì¤‘ìš”: ìŠ¤ì¼€ì¼ë§ì€ ëª¨ë¸ í›ˆë ¨ ì‹œ GroupKFold ë‚´ë¶€ì—ì„œ ìˆ˜í–‰í•˜ì„¸ìš”!")
        print("=" * 60)
        
        return save_dir

if __name__ == "__main__":
    preprocessor = GaitDataPreprocessor()
    save_path = preprocessor.process_and_save()
    print(f"Data Leakageê°€ ë°©ì§€ëœ ì „ì²˜ë¦¬ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")